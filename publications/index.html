<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
  <!-- Meta tags -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>Publications</title>
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="Canyu Zhao">

<!-- Favicon -->
<link rel="icon" type="image/x-icon" href="/images/favicon.ico">


<!-- Open Graph tags -->
<meta property="og:title" content="Publications">
<meta property="og:description" content="">
<meta property="og:type" content="website">
<meta property="og:url" content="http://volcverse.github.io/publications/index.html">

<!-- Twitter Card tags -->
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Publications">
<meta name="twitter:description" content="">

<!-- Academicons -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

<!-- Tailwind CSS -->
<link rel="stylesheet" href="/css/style.css">

<!-- Theme Change Script -->
<script src="https://cdn.jsdelivr.net/npm/theme-change@2.0.2/index.js"></script>

<!-- Altmetric Badge -->
<script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>

<!-- Dimensions Badge -->
<script async src="https://badge.dimensions.ai/badge.js" charset="utf-8"></script>

<!-- Google Adsense -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=your_google_adsense_id" crossorigin="anonymous"></script>

<!-- Umami -->
<script defer src="https://cloud.umami.is/script.js" data-website-id="your_umami_id"></script>

<!-- Baidu Tongji -->
 <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?your_baidu_tongji_id";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Typed -->
<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12"></script>

<!-- Font Awesome -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

<meta name="generator" content="Hexo 8.1.1"></head>
<body class="min-h-screen bg-base-200">

  <!-- Sidebar for Desktop -->
  <aside class="hidden lg:flex fixed left-0 top-0 h-screen w-80 bg-base-200 z-40 flex-col overflow-y-auto">
    <div class="relative h-full">
      <a href="/" class="block no-underline hover:bg-base-100 transition rounded-box">
  <div class="p-4 text-center border-base-300 flex flex-col items-center cursor-pointer hidden lg:flex">
    <div class="avatar mb-4">
      <div class="w-24 rounded-full ring ring-primary ring-offset-base-100">
        <img
  src="/images/avatar.jpg"
  alt="Profile"
  class="transition-transform duration-500 hover:rotate-210 cursor-pointer rounded-full ring ring-primary ring-offset-base-100 ring-offset-2 w-24"
/>

      </div>
    </div>
    <h2 class="text-xl font-bold text-base-content">Canyu Zhao</h2>
    <p class="text-base-content opacity-80"></p>
  </div>
</a>


<nav class="p-4 space-y-2 flex-grow">
  
    
      <a href="/publications/" class="btn btn-ghost w-full justify-start text-base-content hover:text-primary hover:bg-primary/10 transition-all duration-200 flex items-center group">
        <i class="fa-solid fa-newspaper mr-3 text-lg group-hover:scale-110 transition-transform"></i>
        Publications
      </a>
    
      <a href="/projects/" class="btn btn-ghost w-full justify-start text-base-content hover:text-primary hover:bg-primary/10 transition-all duration-200 flex items-center group">
        <i class="fa-solid fa-project-diagram mr-3 text-lg group-hover:scale-110 transition-transform"></i>
        Projects
      </a>
    
      <a href="/cv/" class="btn btn-ghost w-full justify-start text-base-content hover:text-primary hover:bg-primary/10 transition-all duration-200 flex items-center group">
        <i class="fa-solid fa-id-card mr-3 text-lg group-hover:scale-110 transition-transform"></i>
        CV
      </a>
    
      <a href="/notes/" class="btn btn-ghost w-full justify-start text-base-content hover:text-primary hover:bg-primary/10 transition-all duration-200 flex items-center group">
        <i class="fa-solid fa-sticky-note mr-3 text-lg group-hover:scale-110 transition-transform"></i>
        Notes
      </a>
    
      <a href="/contact/" class="btn btn-ghost w-full justify-start text-base-content hover:text-primary hover:bg-primary/10 transition-all duration-200 flex items-center group">
        <i class="fa-solid fa-envelope mr-3 text-lg group-hover:scale-110 transition-transform"></i>
        Contact
      </a>
    
  
</nav>


<div class="p-4 border-t border-base-300">
  <div class="flex justify-center items-center gap-4">
    
      <a href="https://scholar.google.com/citations?user=U_Mjiz8AAAAJ" class="text-base-content hover:text-primary hover:scale-110 transition-all duration-200 flex-shrink-0" target="_blank" rel="noopener noreferrer" title="Google Scholar">
        <i class="fa-brands fa-google-scholar text-2xl block"></i>
      </a>
    
    
      <a href="https://orcid.org/0009-0002-4142-6844" class="text-base-content hover:text-primary hover:scale-110 transition-all duration-200 flex-shrink-0" target="_blank" rel="noopener noreferrer" title="ORCID">
        <i class="fa-brands fa-orcid text-2xl block"></i>
      </a>
    
    
    
      <a href="https://github.com/volcverse" class="text-base-content hover:text-primary hover:scale-110 transition-all duration-200 flex-shrink-0" target="_blank" rel="noopener noreferrer" title="GitHub">
        <i class="fa-brands fa-github text-2xl block"></i>
      </a>
    
    
    
      <a href="https://twitter.com/volcverse" class="text-base-content hover:text-primary hover:scale-110 transition-all duration-200 flex-shrink-0" target="_blank" rel="noopener noreferrer" title="Twitter">
        <i class="fa-brands fa-twitter text-2xl block"></i>
      </a>
    
  </div>
</div>
    </div>

    
  



  </aside>



  <!-- Drawer for Mobile -->
  <div class="drawer lg:hidden">
    <input id="my-drawer" type="checkbox" class="drawer-toggle" />
    <div class="drawer-content flex flex-col">
      <!-- Mobile navbar -->
      <div class="w-full navbar bg-base-300 fixed top-0 left-0 right-0 z-40">
        <label for="my-drawer" class="btn btn-square btn-ghost">
          <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none"
               viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round"
                  stroke-width="2" d="M4 6h16M4 12h16M4 18h16" />
          </svg>
        </label>
        <div class="flex-1">
          <a href="/" class="btn btn-ghost normal-case text-sm lg:text-lg">
          Canyu Zhao
          </a>
        </div>
        <div class="flex-none">
          

<select class="select select-bordered w-full max-w-xs" data-choose-theme>
  
    <option 
      value="light" 
      selected
    >
      üåû Light
    </option>
  
    <option 
      value="dark" 
      
    >
      üåô Dark
    </option>
  
    <option 
      value="cupcake" 
      
    >
      üßÅ Cupcake
    </option>
  
    <option 
      value="bumblebee" 
      
    >
      üêù Bumblebee
    </option>
  
    <option 
      value="emerald" 
      
    >
      üíö Emerald
    </option>
  
    <option 
      value="corporate" 
      
    >
      üè¢ Corporate
    </option>
  
    <option 
      value="synthwave" 
      
    >
      üïπÔ∏è Synthwave
    </option>
  
    <option 
      value="retro" 
      
    >
      üéûÔ∏è Retro
    </option>
  
    <option 
      value="cyberpunk" 
      
    >
      ü§ñ Cyberpunk
    </option>
  
    <option 
      value="valentine" 
      
    >
      ‚ù§Ô∏è Valentine
    </option>
  
    <option 
      value="halloween" 
      
    >
      üéÉ Halloween
    </option>
  
    <option 
      value="garden" 
      
    >
      üåª Garden
    </option>
  
    <option 
      value="forest" 
      
    >
      üå≤ Forest
    </option>
  
    <option 
      value="aqua" 
      
    >
      üíß Aqua
    </option>
  
    <option 
      value="lofi" 
      
    >
      üéß Lofi
    </option>
  
    <option 
      value="pastel" 
      
    >
      üé® Pastel
    </option>
  
    <option 
      value="fantasy" 
      
    >
      üßö Fantasy
    </option>
  
    <option 
      value="wireframe" 
      
    >
      üñºÔ∏è Wireframe
    </option>
  
    <option 
      value="black" 
      
    >
      ‚ö´ Black
    </option>
  
    <option 
      value="luxury" 
      
    >
      üíé Luxury
    </option>
  
    <option 
      value="dracula" 
      
    >
      üßõ Dracula
    </option>
  
    <option 
      value="cmyk" 
      
    >
      üé® Cmyk
    </option>
  
    <option 
      value="autumn" 
      
    >
      üçÇ Autumn
    </option>
  
    <option 
      value="business" 
      
    >
      üíº Business
    </option>
  
    <option 
      value="acid" 
      
    >
      üß™ Acid
    </option>
  
    <option 
      value="lemonade" 
      
    >
      üçã Lemonade
    </option>
  
    <option 
      value="night" 
      
    >
      üåÉ Night
    </option>
  
    <option 
      value="coffee" 
      
    >
      ‚òï Coffee
    </option>
  
    <option 
      value="winter" 
      
    >
      ‚ùÑÔ∏è Winter
    </option>
  
    <option 
      value="dim" 
      
    >
      üí° Dim
    </option>
  
    <option 
      value="nord" 
      
    >
      ‚ùÑÔ∏è Nord
    </option>
  
    <option 
      value="sunset" 
      
    >
      üåÖ Sunset
    </option>
  
    <option 
      value="caramellatte" 
      
    >
      ü•õ Caramellatte
    </option>
  
    <option 
      value="abyss" 
      
    >
      üåå Abyss
    </option>
  
    <option 
      value="silk" 
      
    >
      üßµ Silk
    </option>
  
</select>


        </div>
      </div>
    </div>

    <!-- Sidebar drawer panel for mobile -->
    <div class="drawer-side z-60">
      <label for="my-drawer" class="drawer-overlay"></label>
      <aside class="w-80 bg-base-200 min-h-full overflow-y-auto">
        <div class="relative h-full">
          <a href="/" class="block no-underline hover:bg-base-100 transition rounded-box">
  <div class="p-4 text-center border-base-300 flex flex-col items-center cursor-pointer hidden lg:flex">
    <div class="avatar mb-4">
      <div class="w-24 rounded-full ring ring-primary ring-offset-base-100">
        <img
  src="/images/avatar.jpg"
  alt="Profile"
  class="transition-transform duration-500 hover:rotate-210 cursor-pointer rounded-full ring ring-primary ring-offset-base-100 ring-offset-2 w-24"
/>

      </div>
    </div>
    <h2 class="text-xl font-bold text-base-content">Canyu Zhao</h2>
    <p class="text-base-content opacity-80"></p>
  </div>
</a>


<nav class="p-4 space-y-2 flex-grow">
  
    
      <a href="/publications/" class="btn btn-ghost w-full justify-start text-base-content hover:text-primary hover:bg-primary/10 transition-all duration-200 flex items-center group">
        <i class="fa-solid fa-newspaper mr-3 text-lg group-hover:scale-110 transition-transform"></i>
        Publications
      </a>
    
      <a href="/projects/" class="btn btn-ghost w-full justify-start text-base-content hover:text-primary hover:bg-primary/10 transition-all duration-200 flex items-center group">
        <i class="fa-solid fa-project-diagram mr-3 text-lg group-hover:scale-110 transition-transform"></i>
        Projects
      </a>
    
      <a href="/cv/" class="btn btn-ghost w-full justify-start text-base-content hover:text-primary hover:bg-primary/10 transition-all duration-200 flex items-center group">
        <i class="fa-solid fa-id-card mr-3 text-lg group-hover:scale-110 transition-transform"></i>
        CV
      </a>
    
      <a href="/notes/" class="btn btn-ghost w-full justify-start text-base-content hover:text-primary hover:bg-primary/10 transition-all duration-200 flex items-center group">
        <i class="fa-solid fa-sticky-note mr-3 text-lg group-hover:scale-110 transition-transform"></i>
        Notes
      </a>
    
      <a href="/contact/" class="btn btn-ghost w-full justify-start text-base-content hover:text-primary hover:bg-primary/10 transition-all duration-200 flex items-center group">
        <i class="fa-solid fa-envelope mr-3 text-lg group-hover:scale-110 transition-transform"></i>
        Contact
      </a>
    
  
</nav>


<div class="p-4 border-t border-base-300">
  <div class="flex justify-center items-center gap-4">
    
      <a href="https://scholar.google.com/citations?user=U_Mjiz8AAAAJ" class="text-base-content hover:text-primary hover:scale-110 transition-all duration-200 flex-shrink-0" target="_blank" rel="noopener noreferrer" title="Google Scholar">
        <i class="fa-brands fa-google-scholar text-2xl block"></i>
      </a>
    
    
      <a href="https://orcid.org/0009-0002-4142-6844" class="text-base-content hover:text-primary hover:scale-110 transition-all duration-200 flex-shrink-0" target="_blank" rel="noopener noreferrer" title="ORCID">
        <i class="fa-brands fa-orcid text-2xl block"></i>
      </a>
    
    
    
      <a href="https://github.com/volcverse" class="text-base-content hover:text-primary hover:scale-110 transition-all duration-200 flex-shrink-0" target="_blank" rel="noopener noreferrer" title="GitHub">
        <i class="fa-brands fa-github text-2xl block"></i>
      </a>
    
    
    
      <a href="https://twitter.com/volcverse" class="text-base-content hover:text-primary hover:scale-110 transition-all duration-200 flex-shrink-0" target="_blank" rel="noopener noreferrer" title="Twitter">
        <i class="fa-brands fa-twitter text-2xl block"></i>
      </a>
    
  </div>
</div>
        </div>
      </aside>
    </div>
  </div>

  <!-- Content area for Desktop -->
  <div class="lg:ml-80 pt-20 px-4 py-8">
    <div class="container mx-auto px-4 py-8">
  <h1 class="text-3xl font-bold mb-8 text-base-content">Publications</h1>

  <!-- Filters -->
  <div class="glass-card p-4 mb-8">
    <div class="flex flex-wrap gap-4 items-center">
      <!-- Year Filter -->
      <div class="form-control w-40">
        <select class="select select-bordered" id="yearFilter" name="yearFilter">
          <option value="">All Years</option>
          
            <option value="2025">2025</option>
          
            <option value="2024">2024</option>
          
        </select>
      </div>

      <!-- Category Filter -->
      <div class="form-control w-48">
        <select class="select select-bordered" id="categoryFilter" name="categoryFilter">
          <option value="">All Categories</option>
          
            <option value="3D Display">3D Display</option>
          
            <option value="Diffusion Model">Diffusion Model</option>
          
            <option value="Diffusion Model, Vision Language Model">Diffusion Model, Vision Language Model</option>
          
            <option value="Vision Language Model">Vision Language Model</option>
          
            <option value="Vision Language Model, Robotics">Vision Language Model, Robotics</option>
          
        </select>
      </div>

      <!-- Search -->
      <div class="form-control flex-grow min-w-[200px]">
        <input type="text" placeholder="Search publications..." class="input input-bordered" id="searchInput" />
      </div>
    </div>
  </div>

  <!-- Publications List -->
  <div id="publicationsList" class="space-y-6">
    
      <div class="card glass rounded-box shadow-lg bg-base-100 text-base-content hover:shadow-xl hover:scale-[1.01] transition-all duration-300 border border-base-300 publication-item"
           data-year="2025"
           data-category="diffusion model">
        <div class="card-body p-6">
          <!-- Title and Type Badge -->
          <div class="flex flex-col md:flex-row md:items-start md:justify-between mb-2">
            <h2 class="card-title text-xl font-bold text-primary leading-tight flex-1 mb-1 md:mb-0">
              Diception: A generalist diffusion model for visual perceptual tasks
            </h2>
            <div class="flex items-center gap-2 mt-1 md:mt-0">
              
                <div class="badge badge-secondary badge-lg z-50">Conference</div>
              
              
            </div>
          </div>
  
          <!-- Authors and Metadata -->
          <div class="mb-4">
            
              <p class="text-base font-medium text-base-content/90 mb-3">Canyu Zhao, Yanlong Sun, Mingyu Liu, Huanyi Zheng, Muzhi Zhu, Zhiyue Zhao, Hao Chen, Tong He, Chunhua Shen</p>
            
            
            <div class="flex flex-wrap items-center gap-4 text-sm text-base-content/70">
              
                <span class="flex items-center">
                  <i class="fa-solid fa-building w-4 h-4 mr-1"></i>
                  NeurIPS SpotLight Award
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-calendar-week w-4 h-4 mr-1"></i>
                  2025
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-layer-group w-4 h-4 mr-1"></i>
                  Diffusion Model
                </span>
              
            </div>
            
            <!-- Tags -->
            
              <div class="flex flex-wrap gap-2 mt-3">
                
                  <span class="badge badge-outline badge-sm">Diffusion Model</span>
                
                  <span class="badge badge-outline badge-sm">Generalist Model</span>
                
                  <span class="badge badge-outline badge-sm">Perception</span>
                
              </div>
            
          </div>
  
          <!-- Abstract -->
          
            <div class="prose max-w-none mb-4">
              <p class="text-base-content/80 text-sm leading-relaxed">This paper&#39;s primary objective is to develop a robust generalist perception model capable of addressing multiple tasks under constraints of computational resources and limited training data. We leverage text-to-image diffusion models pre-trained on billions of images and successfully introduce our DICEPTION, a visual generalist model. Exhaustive evaluations demonstrate that DICEPTION effectively tackles diverse perception tasks, even achieving performance comparable to SOTA single-task specialist models. Specifically, we achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). We designed comprehensive experiments on architectures and input paradigms, demonstrating that the key to successfully re-purposing a single diffusion model for multiple perception tasks lies in maximizing the preservation of the pre-trained model&#39;s prior knowledge. Consequently, DICEPTION can be trained with substantially lower computational costs than conventional models requiring training from scratch. Furthermore, adapting DICEPTION to novel tasks is highly efficient, necessitating fine-tuning on as few as 50 images and approximately 1% of its parameters. Finally, we demonstrate that a subtle application of classifier-free guidance can improve the model&#39;s performance on depth and normal estimation. We also show that pixel-aligned training, as is characteristic of perception tasks, significantly enhances the model&#39;s ability to preserve fine details. DICEPTION offers valuable insights and presents a promising direction for the development of advanced diffusion-based visual generalist models.</p>
            </div>
          
  
          <!-- Action Buttons -->
          <div class="card-actions justify-start">
            
            
              <a href="https://arxiv.org/abs/2502.17157" class="btn btn-secondary btn-sm" target="_blank" rel="noopener noreferrer" title="PDF">
                <i class="fa-solid fa-file-pdf"></i>
                PDF
              </a>
            
            
              <a href="https://github.com/aim-uofa/Diception" class="btn btn-accent btn-sm" target="_blank" rel="noopener noreferrer" title="Code">
                <i class="fa-solid fa-code"></i>
                Code
              </a>
            
            
              <a href="https://aim-uofa.github.io/Diception/" class="btn btn-info btn-sm" target="_blank" rel="noopener noreferrer" title="Project Website">
                <i class="fa-solid fa-globe"></i>
                Website
              </a>
            
          </div>
        </div>
      </div>
    
      <div class="card glass rounded-box shadow-lg bg-base-100 text-base-content hover:shadow-xl hover:scale-[1.01] transition-all duration-300 border border-base-300 publication-item"
           data-year="2025"
           data-category="3d display">
        <div class="card-body p-6">
          <!-- Title and Type Badge -->
          <div class="flex flex-col md:flex-row md:items-start md:justify-between mb-2">
            <h2 class="card-title text-xl font-bold text-primary leading-tight flex-1 mb-1 md:mb-0">
              Glasses-free 3D display with ultrawide viewing range using deep learning
            </h2>
            <div class="flex items-center gap-2 mt-1 md:mt-0">
              
                <div class="badge badge-secondary badge-lg z-50">Journal</div>
              
              
                <div class='altmetric-embed z-50 relative' data-badge-type='donut' data-doi='10.1038/s41586-025-09752-y' data-badge-popover="bottom" data-hide-no-mentions="true"></div>
                <span class="__dimensions_badge_embed__ z-50 relative" data-doi='10.1038/s41586-025-09752-y' data-style="small_circle" data-hide-zero-citations="true"></span>
              
            </div>
          </div>
  
          <!-- Authors and Metadata -->
          <div class="mb-4">
            
              <p class="text-base font-medium text-base-content/90 mb-3">Weijie Ma, Zhangrui Zhao, Canyu Zhao, Wanli Ouyang, Han-Sen Zhong</p>
            
            
            <div class="flex flex-wrap items-center gap-4 text-sm text-base-content/70">
              
                <span class="flex items-center">
                  <i class="fa-solid fa-building w-4 h-4 mr-1"></i>
                  Nature
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-calendar-week w-4 h-4 mr-1"></i>
                  2025
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-layer-group w-4 h-4 mr-1"></i>
                  3D Display
                </span>
              
            </div>
            
            <!-- Tags -->
            
              <div class="flex flex-wrap gap-2 mt-3">
                
                  <span class="badge badge-outline badge-sm">Glasses-free 3D Display</span>
                
                  <span class="badge badge-outline badge-sm">Deep Learning</span>
                
              </div>
            
          </div>
  
          <!-- Abstract -->
          
            <div class="prose max-w-none mb-4">
              <p class="text-base-content/80 text-sm leading-relaxed">Glasses-free three-dimensional (3D) displays provide users with an immersive visual experience without the need of any wearable devices. To achieve high-quality 3D imaging, a display should have both large linear dimensions and a wide viewing angle. However, the trade-off between spatial extent and bandwidth of optical systems, the space‚Äìbandwidth product, conventionally constrains the simultaneous maximization of the two. The two most common approaches to 3D displays are holographic and automultiscopic, which, respectively, sacrifice either scale or viewing angle. Recently, some implementations enhanced by artificial intelligence have shown directions to mitigate these constraints, but they still operate within a set space‚Äìbandwidth product. As a result, it remains challenging to fabricate large-scale wide-angle 3D displays. Here we report the realization of a large-scale full-parallax 3D display with seamless viewing beyond 100¬∞, maintained at over 50‚ÄâHz and 1,920‚Äâ√ó‚Äâ1,080 resolution on a low-cost light-field delivery setup. This device, called EyeReal, is realized by accurately modelling binocular view and combining it with a deep-learning real-time optimization, enabling the generation of optimal light-field outputs for each of the eyes. Our device could potentially enable applications in educational tools, 3D design and virtual reality.</p>
            </div>
          
  
          <!-- Action Buttons -->
          <div class="card-actions justify-start">
            
              <a href="https://doi.org/10.1038/s41586-025-09752-y" class="btn btn-primary btn-sm" target="_blank" rel="noopener noreferrer" title="DOI">
                <i class="fa-solid fa-link w-4 h-4 mr-1"></i>
                DOI
              </a>
            
            
              <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12675290/" class="btn btn-secondary btn-sm" target="_blank" rel="noopener noreferrer" title="PDF">
                <i class="fa-solid fa-file-pdf"></i>
                PDF
              </a>
            
            
              <a href="https://github.com/WeijieMax/EyeReal" class="btn btn-accent btn-sm" target="_blank" rel="noopener noreferrer" title="Code">
                <i class="fa-solid fa-code"></i>
                Code
              </a>
            
            
          </div>
        </div>
      </div>
    
      <div class="card glass rounded-box shadow-lg bg-base-100 text-base-content hover:shadow-xl hover:scale-[1.01] transition-all duration-300 border border-base-300 publication-item"
           data-year="2025"
           data-category="diffusion model, vision language model">
        <div class="card-body p-6">
          <!-- Title and Type Badge -->
          <div class="flex flex-col md:flex-row md:items-start md:justify-between mb-2">
            <h2 class="card-title text-xl font-bold text-primary leading-tight flex-1 mb-1 md:mb-0">
              Moviedreamer: Hierarchical generation for coherent long visual sequence
            </h2>
            <div class="flex items-center gap-2 mt-1 md:mt-0">
              
                <div class="badge badge-secondary badge-lg z-50">Conference</div>
              
              
            </div>
          </div>
  
          <!-- Authors and Metadata -->
          <div class="mb-4">
            
              <p class="text-base font-medium text-base-content/90 mb-3">Canyu Zhao*, Mingyu Liu*, Wen Wang*, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, Chunhua Shen</p>
            
            
            <div class="flex flex-wrap items-center gap-4 text-sm text-base-content/70">
              
                <span class="flex items-center">
                  <i class="fa-solid fa-building w-4 h-4 mr-1"></i>
                  ICLR
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-calendar-week w-4 h-4 mr-1"></i>
                  2025
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-layer-group w-4 h-4 mr-1"></i>
                  Diffusion Model, Vision Language Model
                </span>
              
            </div>
            
            <!-- Tags -->
            
              <div class="flex flex-wrap gap-2 mt-3">
                
                  <span class="badge badge-outline badge-sm">Long Video Generation</span>
                
                  <span class="badge badge-outline badge-sm">Diffusion Model</span>
                
                  <span class="badge badge-outline badge-sm">VLM</span>
                
              </div>
            
          </div>
  
          <!-- Abstract -->
          
            <div class="prose max-w-none mb-4">
              <p class="text-base-content/80 text-sm leading-relaxed">Recent advancements in video generation have primarily leveraged diffusion models for short-duration content. However, these approaches often fall short in modeling complex narratives and maintaining character consistency over extended periods, which is essential for long-form video production like movies. We propose MovieDreamer, a novel hierarchical framework that integrates the strengths of autoregressive models with diffusion-based rendering to pioneer long-duration video generation with intricate plot progressions and high visual fidelity. Our approach utilizes autoregressive models for global narrative coherence, predicting sequences of visual tokens that are subsequently transformed into high-quality video frames through diffusion rendering. This method is akin to traditional movie production processes, where complex stories are factorized down into manageable scene capturing. Further, we employ a multimodal script that enriches scene descriptions with detailed character information and visual style, enhancing continuity and character identity across scenes. We present extensive experiments across various movie genres, demonstrating that our approach not only achieves superior visual and narrative quality but also effectively extends the duration of generated content significantly beyond current capabilities.</p>
            </div>
          
  
          <!-- Action Buttons -->
          <div class="card-actions justify-start">
            
            
              <a href="https://arxiv.org/pdf/2407.16655?" class="btn btn-secondary btn-sm" target="_blank" rel="noopener noreferrer" title="PDF">
                <i class="fa-solid fa-file-pdf"></i>
                PDF
              </a>
            
            
            
              <a href="https://aim-uofa.github.io/MovieDreamer/" class="btn btn-info btn-sm" target="_blank" rel="noopener noreferrer" title="Project Website">
                <i class="fa-solid fa-globe"></i>
                Website
              </a>
            
          </div>
        </div>
      </div>
    
      <div class="card glass rounded-box shadow-lg bg-base-100 text-base-content hover:shadow-xl hover:scale-[1.01] transition-all duration-300 border border-base-300 publication-item"
           data-year="2024"
           data-category="diffusion model">
        <div class="card-body p-6">
          <!-- Title and Type Badge -->
          <div class="flex flex-col md:flex-row md:items-start md:justify-between mb-2">
            <h2 class="card-title text-xl font-bold text-primary leading-tight flex-1 mb-1 md:mb-0">
              Autostory: Generating diverse storytelling images with minimal human efforts
            </h2>
            <div class="flex items-center gap-2 mt-1 md:mt-0">
              
                <div class="badge badge-secondary badge-lg z-50">Journal</div>
              
              
                <div class='altmetric-embed z-50 relative' data-badge-type='donut' data-doi='s11263-024-02309-y' data-badge-popover="bottom" data-hide-no-mentions="true"></div>
                <span class="__dimensions_badge_embed__ z-50 relative" data-doi='s11263-024-02309-y' data-style="small_circle" data-hide-zero-citations="true"></span>
              
            </div>
          </div>
  
          <!-- Authors and Metadata -->
          <div class="mb-4">
            
              <p class="text-base font-medium text-base-content/90 mb-3">Wen Wang*, Canyu Zhao*, Hao Chen, Zhekai Chen, Kecheng Zheng, Chunhua Shen</p>
            
            
            <div class="flex flex-wrap items-center gap-4 text-sm text-base-content/70">
              
                <span class="flex items-center">
                  <i class="fa-solid fa-building w-4 h-4 mr-1"></i>
                  International Journal of Computer Vision
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-calendar-week w-4 h-4 mr-1"></i>
                  2024
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-layer-group w-4 h-4 mr-1"></i>
                  Diffusion Model
                </span>
              
            </div>
            
            <!-- Tags -->
            
              <div class="flex flex-wrap gap-2 mt-3">
                
                  <span class="badge badge-outline badge-sm">Diffusion Model</span>
                
                  <span class="badge badge-outline badge-sm">Visual Story Generation</span>
                
              </div>
            
          </div>
  
          <!-- Abstract -->
          
            <div class="prose max-w-none mb-4">
              <p class="text-base-content/80 text-sm leading-relaxed">Story visualization aims to generate a series of images that match the story described in texts, and it requires the generated images to satisfy high quality, alignment with the text description, and consistency in character identities. Given the complexity of story visualization, existing methods drastically simplify the problem by considering only a few specific characters and scenarios, or requiring the users to provide per-image control conditions such as sketches. However, these simplifications render these methods incompetent for real applications. To this end, we propose an automated story visualization system that can effectively generate diverse, high-quality, and consistent sets of story images, with minimal human interactions. Specifically, we utilize the comprehension and planning capabilities of large language models for layout planning, and then leverage large-scale text-to-image models to generate sophisticated story images based on the layout. We empirically find that sparse control conditions, such as bounding boxes, are suitable for layout planning, while dense control conditions, e.g., sketches, and keypoints, are suitable for generating high-quality image content. To obtain the best of both worlds, we devise a dense condition generation module to transform simple bounding box layouts into sketch or keypoint control conditions for final image generation, which not only improves the image quality but also allows easy and intuitive user interactions. In addition, we propose a simple yet effective method to generate multi-view consistent character images, eliminating the reliance on human labor to collect or draw character images. This allows our method to obtain consistent story visualization even when only texts are provided as input. Both qualitative and quantitative experiments demonstrate the superiority of our method.</p>
            </div>
          
  
          <!-- Action Buttons -->
          <div class="card-actions justify-start">
            
              <a href="https://doi.org/s11263-024-02309-y" class="btn btn-primary btn-sm" target="_blank" rel="noopener noreferrer" title="DOI">
                <i class="fa-solid fa-link w-4 h-4 mr-1"></i>
                DOI
              </a>
            
            
              <a href="https://link.springer.com/article/10.1007/s11263-024-02309-y" class="btn btn-secondary btn-sm" target="_blank" rel="noopener noreferrer" title="PDF">
                <i class="fa-solid fa-file-pdf"></i>
                PDF
              </a>
            
            
            
          </div>
        </div>
      </div>
    
      <div class="card glass rounded-box shadow-lg bg-base-100 text-base-content hover:shadow-xl hover:scale-[1.01] transition-all duration-300 border border-base-300 publication-item"
           data-year="2024"
           data-category="diffusion model">
        <div class="card-body p-6">
          <!-- Title and Type Badge -->
          <div class="flex flex-col md:flex-row md:items-start md:justify-between mb-2">
            <h2 class="card-title text-xl font-bold text-primary leading-tight flex-1 mb-1 md:mb-0">
              Freecustom: Tuning-free customized image generation for multi-concept composition
            </h2>
            <div class="flex items-center gap-2 mt-1 md:mt-0">
              
                <div class="badge badge-secondary badge-lg z-50">Conference</div>
              
              
            </div>
          </div>
  
          <!-- Authors and Metadata -->
          <div class="mb-4">
            
              <p class="text-base font-medium text-base-content/90 mb-3">Ganggui Ding*, Canyu Zhao*, Wen Wang*, Zhen Yang, Zide Liu, Hao Chen, Chunhua Shen</p>
            
            
            <div class="flex flex-wrap items-center gap-4 text-sm text-base-content/70">
              
                <span class="flex items-center">
                  <i class="fa-solid fa-building w-4 h-4 mr-1"></i>
                  CVPR
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-calendar-week w-4 h-4 mr-1"></i>
                  2024
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-layer-group w-4 h-4 mr-1"></i>
                  Diffusion Model
                </span>
              
            </div>
            
            <!-- Tags -->
            
              <div class="flex flex-wrap gap-2 mt-3">
                
                  <span class="badge badge-outline badge-sm">Diffusion Model</span>
                
                  <span class="badge badge-outline badge-sm">Image Customization</span>
                
              </div>
            
          </div>
  
          <!-- Abstract -->
          
            <div class="prose max-w-none mb-4">
              <p class="text-base-content/80 text-sm leading-relaxed">Benefiting from large-scale pre-trained text-to-image (T2I) generative models impressive progress has been achieved in customized image generation which aims to generate user-specified concepts. Existing approaches have extensively focused on single-concept customization and still encounter challenges when it comes to complex scenarios that involve combining multiple concepts. These approaches often require retraining/fine-tuning using a few images leading to time-consuming training processes and impeding their swift implementation. Furthermore the reliance on multiple images to represent a singular concept increases the difficulty of customization. To this end we propose FreeCustom a novel tuning-free method to generate customized images of multi-concept composition based on reference concepts using only one image per concept as input. Specifically we introduce a new multi-reference self-attention (MRSA) mechanism and a weighted mask strategy that enables the generated image to access and focus more on the reference concepts. In addition MRSA leverages our key finding that input concepts are better preserved when providing images with context interactions. Experiments show that our method&#39;s produced images are consistent with the given concepts and better aligned with the input text. Our method outperforms or performs on par with other training-based methods in terms of multi-concept composition and single-concept customization but is simpler.</p>
            </div>
          
  
          <!-- Action Buttons -->
          <div class="card-actions justify-start">
            
            
              <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Ding_FreeCustom_Tuning-Free_Customized_Image_Generation_for_Multi-Concept_Composition_CVPR_2024_paper.html" class="btn btn-secondary btn-sm" target="_blank" rel="noopener noreferrer" title="PDF">
                <i class="fa-solid fa-file-pdf"></i>
                PDF
              </a>
            
            
              <a href="https://github.com/aim-uofa/FreeCustom" class="btn btn-accent btn-sm" target="_blank" rel="noopener noreferrer" title="Code">
                <i class="fa-solid fa-code"></i>
                Code
              </a>
            
            
          </div>
        </div>
      </div>
    
      <div class="card glass rounded-box shadow-lg bg-base-100 text-base-content hover:shadow-xl hover:scale-[1.01] transition-all duration-300 border border-base-300 publication-item"
           data-year="2025"
           data-category="vision language model">
        <div class="card-body p-6">
          <!-- Title and Type Badge -->
          <div class="flex flex-col md:flex-row md:items-start md:justify-between mb-2">
            <h2 class="card-title text-xl font-bold text-primary leading-tight flex-1 mb-1 md:mb-0">
              Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration
            </h2>
            <div class="flex items-center gap-2 mt-1 md:mt-0">
              
                <div class="badge badge-secondary badge-lg z-50">Conference</div>
              
              
            </div>
          </div>
  
          <!-- Authors and Metadata -->
          <div class="mb-4">
            
              <p class="text-base font-medium text-base-content/90 mb-3">Hao Zhong*, Muzhi Zhu*, Zongze Du, Zheng Huang, Canyu Zhao, Mingyu Liu, Wen Wang, Hao Chen, Chunhua Shen</p>
            
            
            <div class="flex flex-wrap items-center gap-4 text-sm text-base-content/70">
              
                <span class="flex items-center">
                  <i class="fa-solid fa-building w-4 h-4 mr-1"></i>
                  NeurIPS
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-calendar-week w-4 h-4 mr-1"></i>
                  2025
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-layer-group w-4 h-4 mr-1"></i>
                  Vision Language Model
                </span>
              
            </div>
            
            <!-- Tags -->
            
              <div class="flex flex-wrap gap-2 mt-3">
                
                  <span class="badge badge-outline badge-sm">Vision Language Model</span>
                
                  <span class="badge badge-outline badge-sm">RL</span>
                
              </div>
            
          </div>
  
          <!-- Abstract -->
          
            <div class="prose max-w-none mb-4">
              <p class="text-base-content/80 text-sm leading-relaxed">Long-horizon video-audio reasoning and fine-grained pixel understanding impose conflicting requirements on omnimodal models: dense temporal coverage demands many low-resolution frames, whereas precise grounding calls for high-resolution inputs. We tackle this trade-off with a two-system architecture: a Global Reasoning System selects informative keyframes and rewrites the task at low spatial cost, while a Detail Understanding System performs pixel-level grounding on the selected high-resolution snippets. Because ``optimal&#39;&#39; keyframe selection and reformulation are ambiguous and hard to supervise, we formulate them as a reinforcement learning (RL) problem and present Omni-R1, an end-to-end RL framework built on Group Relative Policy Optimization. Omni-R1 trains the Global Reasoning System through hierarchical rewards obtained via online collaboration with the Detail Understanding System, requiring only one epoch of RL on small task splits. Experiments on two challenging benchmarks, namely Referring Audio-Visual Segmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show that Omni-R1 not only surpasses strong supervised baselines but also outperforms specialized state-of-the-art models, while substantially improving out-of-domain generalization and mitigating multimodal hallucination. Our results demonstrate the first successful application of RL to large-scale omnimodal reasoning and highlight a scalable path toward universally foundation models.</p>
            </div>
          
  
          <!-- Action Buttons -->
          <div class="card-actions justify-start">
            
            
              <a href="https://arxiv.org/abs/2505.20256" class="btn btn-secondary btn-sm" target="_blank" rel="noopener noreferrer" title="PDF">
                <i class="fa-solid fa-file-pdf"></i>
                PDF
              </a>
            
            
              <a href="https://github.com/aim-uofa/Omni-R1" class="btn btn-accent btn-sm" target="_blank" rel="noopener noreferrer" title="Code">
                <i class="fa-solid fa-code"></i>
                Code
              </a>
            
            
          </div>
        </div>
      </div>
    
      <div class="card glass rounded-box shadow-lg bg-base-100 text-base-content hover:shadow-xl hover:scale-[1.01] transition-all duration-300 border border-base-300 publication-item"
           data-year="2025"
           data-category="vision language model">
        <div class="card-body p-6">
          <!-- Title and Type Badge -->
          <div class="flex flex-col md:flex-row md:items-start md:justify-between mb-2">
            <h2 class="card-title text-xl font-bold text-primary leading-tight flex-1 mb-1 md:mb-0">
              Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO
            </h2>
            <div class="flex items-center gap-2 mt-1 md:mt-0">
              
                <div class="badge badge-secondary badge-lg z-50">Arxiv</div>
              
              
            </div>
          </div>
  
          <!-- Authors and Metadata -->
          <div class="mb-4">
            
              <p class="text-base font-medium text-base-content/90 mb-3">Muzhi Zhu*, Hao Zhong*, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, Chunhua Shen</p>
            
            
            <div class="flex flex-wrap items-center gap-4 text-sm text-base-content/70">
              
                <span class="flex items-center">
                  <i class="fa-solid fa-building w-4 h-4 mr-1"></i>
                  Arxiv
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-calendar-week w-4 h-4 mr-1"></i>
                  2025
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-layer-group w-4 h-4 mr-1"></i>
                  Vision Language Model
                </span>
              
            </div>
            
            <!-- Tags -->
            
              <div class="flex flex-wrap gap-2 mt-3">
                
                  <span class="badge badge-outline badge-sm">Vision Language Model</span>
                
                  <span class="badge badge-outline badge-sm">RL</span>
                
              </div>
            
          </div>
  
          <!-- Abstract -->
          
            <div class="prose max-w-none mb-4">
              <p class="text-base-content/80 text-sm leading-relaxed">Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model&#39;s zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.</p>
            </div>
          
  
          <!-- Action Buttons -->
          <div class="card-actions justify-start">
            
            
              <a href="https://arxiv.org/abs/2505.21457" class="btn btn-secondary btn-sm" target="_blank" rel="noopener noreferrer" title="PDF">
                <i class="fa-solid fa-file-pdf"></i>
                PDF
              </a>
            
            
              <a href="https://github.com/aim-uofa/Active-o3" class="btn btn-accent btn-sm" target="_blank" rel="noopener noreferrer" title="Code">
                <i class="fa-solid fa-code"></i>
                Code
              </a>
            
            
          </div>
        </div>
      </div>
    
      <div class="card glass rounded-box shadow-lg bg-base-100 text-base-content hover:shadow-xl hover:scale-[1.01] transition-all duration-300 border border-base-300 publication-item"
           data-year="2025"
           data-category="diffusion model">
        <div class="card-body p-6">
          <!-- Title and Type Badge -->
          <div class="flex flex-col md:flex-row md:items-start md:justify-between mb-2">
            <h2 class="card-title text-xl font-bold text-primary leading-tight flex-1 mb-1 md:mb-0">
              Tinker: Diffusion&#39;s Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization
            </h2>
            <div class="flex items-center gap-2 mt-1 md:mt-0">
              
                <div class="badge badge-secondary badge-lg z-50">Arxiv</div>
              
              
            </div>
          </div>
  
          <!-- Authors and Metadata -->
          <div class="mb-4">
            
              <p class="text-base font-medium text-base-content/90 mb-3">Canyu Zhao*, Xiaoman Li*, Tianjian Feng, Zhiyue Zhao, Hao Chen, Chunhua Shen</p>
            
            
            <div class="flex flex-wrap items-center gap-4 text-sm text-base-content/70">
              
                <span class="flex items-center">
                  <i class="fa-solid fa-building w-4 h-4 mr-1"></i>
                  Arxiv
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-calendar-week w-4 h-4 mr-1"></i>
                  2025
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-layer-group w-4 h-4 mr-1"></i>
                  Diffusion Model
                </span>
              
            </div>
            
            <!-- Tags -->
            
              <div class="flex flex-wrap gap-2 mt-3">
                
                  <span class="badge badge-outline badge-sm">Diffusion Model</span>
                
                  <span class="badge badge-outline badge-sm">3D Editing</span>
                
              </div>
            
          </div>
  
          <!-- Abstract -->
          
            <div class="prose max-w-none mb-4">
              <p class="text-base-content/80 text-sm leading-relaxed">We introduce Tinker, a versatile framework for high-fidelity 3D editing that operates in both one-shot and few-shot regimes without any per-scene finetuning. Unlike prior techniques that demand extensive per-scene optimization to ensure multi-view consistency or to produce dozens of consistent edited input views, Tinker delivers robust, multi-view consistent edits from as few as one or two images. This capability stems from repurposing pretrained diffusion models, which unlocks their latent 3D awareness. To drive research in this space, we curate the first large-scale multi-view editing dataset and data pipeline, spanning diverse scenes and styles. Building on this dataset, we develop our framework capable of generating multi-view consistent edited views without per-scene training, which consists of two novel components: (1) Referring multi-view editor: Enables precise, reference-driven edits that remain coherent across all viewpoints. (2) Any-view-to-video synthesizer: Leverages spatial-temporal priors from video diffusion to perform high-quality scene completion and novel-view generation even from sparse inputs. Through extensive experiments, Tinker significantly reduces the barrier to generalizable 3D content creation, achieving state-of-the-art performance on editing, novel-view synthesis, and rendering enhancement tasks. We believe that Tinker represents a key step towards truly scalable, zero-shot 3D editing.</p>
            </div>
          
  
          <!-- Action Buttons -->
          <div class="card-actions justify-start">
            
            
              <a href="https://arxiv.org/abs/2508.14811" class="btn btn-secondary btn-sm" target="_blank" rel="noopener noreferrer" title="PDF">
                <i class="fa-solid fa-file-pdf"></i>
                PDF
              </a>
            
            
            
          </div>
        </div>
      </div>
    
      <div class="card glass rounded-box shadow-lg bg-base-100 text-base-content hover:shadow-xl hover:scale-[1.01] transition-all duration-300 border border-base-300 publication-item"
           data-year="2025"
           data-category="vision language model, robotics">
        <div class="card-body p-6">
          <!-- Title and Type Badge -->
          <div class="flex flex-col md:flex-row md:items-start md:justify-between mb-2">
            <h2 class="card-title text-xl font-bold text-primary leading-tight flex-1 mb-1 md:mb-0">
              StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation
            </h2>
            <div class="flex items-center gap-2 mt-1 md:mt-0">
              
                <div class="badge badge-secondary badge-lg z-50">Arxiv</div>
              
              
            </div>
          </div>
  
          <!-- Authors and Metadata -->
          <div class="mb-4">
            
              <p class="text-base font-medium text-base-content/90 mb-3">Mingyu Liu, Jiuhe Shu, Hui Chen, Zeju Li, Canyu Zhao, Jiange Yang, Shenyuan Gao, Hao Chen, Chunhua Shen</p>
            
            
            <div class="flex flex-wrap items-center gap-4 text-sm text-base-content/70">
              
                <span class="flex items-center">
                  <i class="fa-solid fa-building w-4 h-4 mr-1"></i>
                  Arxiv
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-calendar-week w-4 h-4 mr-1"></i>
                  2025
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-layer-group w-4 h-4 mr-1"></i>
                  Vision Language Model, Robotics
                </span>
              
            </div>
            
            <!-- Tags -->
            
              <div class="flex flex-wrap gap-2 mt-3">
                
                  <span class="badge badge-outline badge-sm">Vision Language Model</span>
                
                  <span class="badge badge-outline badge-sm">Robotics</span>
                
              </div>
            
          </div>
  
          <!-- Abstract -->
          
            <div class="prose max-w-none mb-4">
              <p class="text-base-content/80 text-sm leading-relaxed">A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video.</p>
            </div>
          
  
          <!-- Action Buttons -->
          <div class="card-actions justify-start">
            
            
              <a href="https://arxiv.org/abs/2505.21457" class="btn btn-secondary btn-sm" target="_blank" rel="noopener noreferrer" title="PDF">
                <i class="fa-solid fa-file-pdf"></i>
                PDF
              </a>
            
            
            
          </div>
        </div>
      </div>
    
      <div class="card glass rounded-box shadow-lg bg-base-100 text-base-content hover:shadow-xl hover:scale-[1.01] transition-all duration-300 border border-base-300 publication-item"
           data-year="2025"
           data-category="vision language model, robotics">
        <div class="card-body p-6">
          <!-- Title and Type Badge -->
          <div class="flex flex-col md:flex-row md:items-start md:justify-between mb-2">
            <h2 class="card-title text-xl font-bold text-primary leading-tight flex-1 mb-1 md:mb-0">
              NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation
            </h2>
            <div class="flex items-center gap-2 mt-1 md:mt-0">
              
                <div class="badge badge-secondary badge-lg z-50">Arxiv</div>
              
              
            </div>
          </div>
  
          <!-- Authors and Metadata -->
          <div class="mb-4">
            
              <p class="text-base font-medium text-base-content/90 mb-3">Zheng Huang, Mingyu Liu, Xiaoyi Lin, Muzhi Zhu, Canyu Zhao, Zongze Du, Xiaoman Li, Yiduo Jia, Hao Zhong, Hao Chen, Chunhua Shen</p>
            
            
            <div class="flex flex-wrap items-center gap-4 text-sm text-base-content/70">
              
                <span class="flex items-center">
                  <i class="fa-solid fa-building w-4 h-4 mr-1"></i>
                  Arxiv
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-calendar-week w-4 h-4 mr-1"></i>
                  2025
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-layer-group w-4 h-4 mr-1"></i>
                  Vision Language Model, Robotics
                </span>
              
            </div>
            
            <!-- Tags -->
            
              <div class="flex flex-wrap gap-2 mt-3">
                
                  <span class="badge badge-outline badge-sm">Vision Language Model</span>
                
                  <span class="badge badge-outline badge-sm">Robotics</span>
                
              </div>
            
          </div>
  
          <!-- Abstract -->
          
            <div class="prose max-w-none mb-4">
              <p class="text-base-content/80 text-sm leading-relaxed">Vision-Language-Action (VLA) models represent a pivotal advance in embodied intelligence, yet they confront critical barriers to real-world deployment, most notably catastrophic forgetting. This issue stems from their overreliance on continuous action sequences or action chunks, which inadvertently create isolated data silos that disrupt knowledge retention across tasks. To tackle these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA) framework: a novel approach that narrows its focus to sparse trajectories, thereby avoiding the catastrophic forgetting associated with dense trajectory fine-tuning. A key innovation of NoTVLA lies in its trajectory planning strategy: instead of centering on the target object&#39;s trajectory, it leverages temporal compression and spatial reasoning pruning specifically for the robot end effector&#39;s trajectory. Furthermore, training is conducted using these sparse trajectories rather than dense action trajectories, an optimization that delivers remarkable practical advantages with better performance in zero-shot. In multi-task evaluation scenarios, NoTVLA achieves superior performance and generalization compared to pi0 while operating under two critical constraints: it uses over an order of magnitude less computing power than pi0 and requires no wrist-mounted camera. This design ensures that NoTVLA&#39;s operational accuracy closely approximates that of single-task expert models. Crucially, it also preserves the model&#39;s inherent language capabilities, enabling zero-shot generalization in specific scenarios, supporting unified model deployment across multiple robot platforms, and fostering a degree of generalization even when perceiving tasks from novel perspectives.</p>
            </div>
          
  
          <!-- Action Buttons -->
          <div class="card-actions justify-start">
            
            
              <a href="https://arxiv.org/abs/2510.03895" class="btn btn-secondary btn-sm" target="_blank" rel="noopener noreferrer" title="PDF">
                <i class="fa-solid fa-file-pdf"></i>
                PDF
              </a>
            
            
            
          </div>
        </div>
      </div>
    
      <div class="card glass rounded-box shadow-lg bg-base-100 text-base-content hover:shadow-xl hover:scale-[1.01] transition-all duration-300 border border-base-300 publication-item"
           data-year="2025"
           data-category="vision language model, robotics">
        <div class="card-body p-6">
          <!-- Title and Type Badge -->
          <div class="flex flex-col md:flex-row md:items-start md:justify-between mb-2">
            <h2 class="card-title text-xl font-bold text-primary leading-tight flex-1 mb-1 md:mb-0">
              Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert
            </h2>
            <div class="flex items-center gap-2 mt-1 md:mt-0">
              
                <div class="badge badge-secondary badge-lg z-50">Arxiv</div>
              
              
            </div>
          </div>
  
          <!-- Authors and Metadata -->
          <div class="mb-4">
            
              <p class="text-base font-medium text-base-content/90 mb-3">Mingyu Liu, Zheng Huang, Xiaoyi Lin, Muzhi Zhu, Canyu Zhao, Zongze Du, Yating Wang, Haoyi Zhu, Hao Chen, Chunhua Shen</p>
            
            
            <div class="flex flex-wrap items-center gap-4 text-sm text-base-content/70">
              
                <span class="flex items-center">
                  <i class="fa-solid fa-building w-4 h-4 mr-1"></i>
                  Arxiv
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-calendar-week w-4 h-4 mr-1"></i>
                  2025
                </span>
              
              
                <span class="flex items-center">
                  <i class="fa-solid fa-layer-group w-4 h-4 mr-1"></i>
                  Vision Language Model, Robotics
                </span>
              
            </div>
            
            <!-- Tags -->
            
              <div class="flex flex-wrap gap-2 mt-3">
                
                  <span class="badge badge-outline badge-sm">Vision Language Model</span>
                
                  <span class="badge badge-outline badge-sm">Robotics</span>
                
              </div>
            
          </div>
  
          <!-- Abstract -->
          
            <div class="prose max-w-none mb-4">
              <p class="text-base-content/80 text-sm leading-relaxed">Although Vision-Language Models (VLM) have demonstrated impressive planning and reasoning capabilities, translating these abilities into the physical world introduces significant challenges. Conventional Vision-Language-Action (VLA) models, which integrate reasoning and action into a monolithic architecture, generalize poorly because they are constrained by scarce, narrow-domain data. While recent dual-system approaches attempt to decouple thinking from acting, they are often constrained by semantic ambiguities within the action module. This ambiguity makes large-scale, cross-task training infeasible. Consequently, these systems typically necessitate fine-tuning on newly collected data when deployed to novel environments, and the cooperation mechanism between the two systems remains ill-defined. To address these limitations, we introduce, for the first time, a framework centered around a generalizable action expert. Our approach utilizes sparse 3D trajectories as an intermediate representation, effectively bridging the high-level planning capabilities of the VLM with the low-level physical action module. During the planning phase, the VLM is only required to generate coarse 3D waypoints. These waypoints are then processed by our generalizable action expert, which refines them into dense, executable action sequences by sampling real-time point cloud observations of the environment. To promote training efficiency and robust generalization, we introduce a novel Action Pre-training, Pointcloud Fine-tuning paradigm. Our method combines the broad generalization capabilities of VLMs in visual understanding and planning with the fine-grained, action-level generalization of action expert.</p>
            </div>
          
  
          <!-- Action Buttons -->
          <div class="card-actions justify-start">
            
            
              <a href="https://arxiv.org/abs/2510.03896" class="btn btn-secondary btn-sm" target="_blank" rel="noopener noreferrer" title="PDF">
                <i class="fa-solid fa-file-pdf"></i>
                PDF
              </a>
            
            
            
          </div>
        </div>
      </div>
    
  </div>
  
</div>

<script>
  const yearFilter = document.getElementById('yearFilter');
  const categoryFilter = document.getElementById('categoryFilter');
  const searchInput = document.getElementById('searchInput');
  const publications = document.querySelectorAll('.publication-item');

  function filterPublications() {
    const yearVal = yearFilter.value;
    const categoryVal = categoryFilter.value.toLowerCase();
    const searchVal = searchInput.value.toLowerCase();
    
    let visibleCount = 0;

    publications.forEach(pub => {
      const pubYear = pub.getAttribute('data-year');
      const pubCategory = pub.getAttribute('data-category') || '';
      const titleElement = pub.querySelector('.card-title');
      const authorsElement = pub.querySelector('p.text-base');
      
      const title = titleElement ? titleElement.textContent.toLowerCase() : '';
      const authors = authorsElement ? authorsElement.textContent.toLowerCase() : '';
      const venue = pub.querySelector('em') ? pub.querySelector('em').textContent.toLowerCase() : '';
      const abstract = pub.querySelector('.prose p') ? pub.querySelector('.prose p').textContent.toLowerCase() : '';

      const matchesYear = !yearVal || pubYear === yearVal;
      const matchesCategory = !categoryVal || pubCategory.toLowerCase().includes(categoryVal);
      const matchesSearch = !searchVal || 
                           title.includes(searchVal) || 
                           authors.includes(searchVal) ||
                           venue.includes(searchVal) ||
                           abstract.includes(searchVal);

      if (matchesYear && matchesCategory && matchesSearch) {
        pub.style.display = 'block';
        visibleCount++;
      } else {
        pub.style.display = 'none';
      }
    });
    
    // Update results count (if you want to add this)
    updateResultsCount(visibleCount, searchVal, yearVal, categoryVal);
  }
  
  function updateResultsCount(count, search, year, category) {
    // You can add a results counter element if needed
    const filters = [search, year, category].filter(Boolean);
    console.log(`Showing ${count} publications${filters.length ? ' (filtered)' : ''}`);
  }

  yearFilter.addEventListener('change', filterPublications);
  categoryFilter.addEventListener('change', filterPublications);
  searchInput.addEventListener('input', filterPublications);

  // ÂàùÂßãÂåñÊòæÁ§∫ÂÖ®ÈÉ®
  filterPublications();
</script>

  </div>

  <!-- Fixed top navbar for desktop -->
  <div class="hidden lg:flex fixed top-0 left-80 right-0 z-50 bg-base-300 h-16 items-center px-4">
    <div class="flex-1">
      <a href="/" class="btn btn-ghost normal-case text-xl">
      Canyu Zhao
      </a>
    </div>
    <div class="flex-none">
      

<select class="select select-bordered w-full max-w-xs" data-choose-theme>
  
    <option 
      value="light" 
      selected
    >
      üåû Light
    </option>
  
    <option 
      value="dark" 
      
    >
      üåô Dark
    </option>
  
    <option 
      value="cupcake" 
      
    >
      üßÅ Cupcake
    </option>
  
    <option 
      value="bumblebee" 
      
    >
      üêù Bumblebee
    </option>
  
    <option 
      value="emerald" 
      
    >
      üíö Emerald
    </option>
  
    <option 
      value="corporate" 
      
    >
      üè¢ Corporate
    </option>
  
    <option 
      value="synthwave" 
      
    >
      üïπÔ∏è Synthwave
    </option>
  
    <option 
      value="retro" 
      
    >
      üéûÔ∏è Retro
    </option>
  
    <option 
      value="cyberpunk" 
      
    >
      ü§ñ Cyberpunk
    </option>
  
    <option 
      value="valentine" 
      
    >
      ‚ù§Ô∏è Valentine
    </option>
  
    <option 
      value="halloween" 
      
    >
      üéÉ Halloween
    </option>
  
    <option 
      value="garden" 
      
    >
      üåª Garden
    </option>
  
    <option 
      value="forest" 
      
    >
      üå≤ Forest
    </option>
  
    <option 
      value="aqua" 
      
    >
      üíß Aqua
    </option>
  
    <option 
      value="lofi" 
      
    >
      üéß Lofi
    </option>
  
    <option 
      value="pastel" 
      
    >
      üé® Pastel
    </option>
  
    <option 
      value="fantasy" 
      
    >
      üßö Fantasy
    </option>
  
    <option 
      value="wireframe" 
      
    >
      üñºÔ∏è Wireframe
    </option>
  
    <option 
      value="black" 
      
    >
      ‚ö´ Black
    </option>
  
    <option 
      value="luxury" 
      
    >
      üíé Luxury
    </option>
  
    <option 
      value="dracula" 
      
    >
      üßõ Dracula
    </option>
  
    <option 
      value="cmyk" 
      
    >
      üé® Cmyk
    </option>
  
    <option 
      value="autumn" 
      
    >
      üçÇ Autumn
    </option>
  
    <option 
      value="business" 
      
    >
      üíº Business
    </option>
  
    <option 
      value="acid" 
      
    >
      üß™ Acid
    </option>
  
    <option 
      value="lemonade" 
      
    >
      üçã Lemonade
    </option>
  
    <option 
      value="night" 
      
    >
      üåÉ Night
    </option>
  
    <option 
      value="coffee" 
      
    >
      ‚òï Coffee
    </option>
  
    <option 
      value="winter" 
      
    >
      ‚ùÑÔ∏è Winter
    </option>
  
    <option 
      value="dim" 
      
    >
      üí° Dim
    </option>
  
    <option 
      value="nord" 
      
    >
      ‚ùÑÔ∏è Nord
    </option>
  
    <option 
      value="sunset" 
      
    >
      üåÖ Sunset
    </option>
  
    <option 
      value="caramellatte" 
      
    >
      ü•õ Caramellatte
    </option>
  
    <option 
      value="abyss" 
      
    >
      üåå Abyss
    </option>
  
    <option 
      value="silk" 
      
    >
      üßµ Silk
    </option>
  
</select>


    </div>
  </div>

  <!-- Footer, È°µÈù¢Âè™Âá∫Áé∞‰∏ÄÊ¨° -->
  <footer class="mt-auto">
    <!-- Footer -->
<footer class="footer footer-center sm:footer-horizontal p-4 bg-base-200 text-base-content">
  <div class="grid grid-flow-col gap-4">
    <div class="text-center">
      <span>¬© 2025 Canyu Zhao</span>
      <span>¬∑</span>
      <span>Powered by <a href="https://hexo.io/" target="_blank" rel="noopener noreferrer" class="link link-info no-underline">Hexo</a></span>
      <span>¬∑</span>
      <span>
        Theme: 
        <a href="https://github.com/jiehua1995/hexo-theme-researcher" target="_blank" rel="noopener noreferrer" class="link link-info no-underline">
          Researcher v<span id="current-theme-version"></span>
          <span id="theme-update-message" class="text-sm" style="display:none;">(<i class="fa-solid fa-cloud-arrow-up"></i> Update Available!)</span>
        </a>
      </span>
    </div>
  </div>
</footer>

<script>
(function() {
  const versionEl = document.getElementById('current-theme-version');
  const updateMessageEl = document.getElementById('theme-update-message');

  // ÂΩìÂâçÁâàÊú¨Âè∑ÔºåÂèØ‰ª•ÊîπÊàê‰Ω†Â∏åÊúõÂâçÁ´ØÂ±ïÁ§∫ÁöÑ
  const currentVersion = '0.1.5';
  versionEl.textContent = currentVersion;

  // GitHub API Ëé∑ÂèñÊúÄÊñ∞ release
  fetch('https://api.github.com/repos/jiehua1995/hexo-theme-researcher/releases/latest')
    .then(res => res.json())
    .then(data => {
      if (data && data.tag_name) {
        const latestVersion = data.tag_name.replace(/^v/, '');
        if (latestVersion !== currentVersion) {
          updateMessageEl.style.display = 'inline';
          updateMessageEl.title = `Latest version: ${latestVersion}`;
        }
      }
    })
    .catch(err => console.error('Failed to fetch latest theme version:', err));
})();
</script>

  </footer>

  <!-- back to top -->
  <!-- ËøîÂõûÈ°∂ÈÉ®ÊåâÈíÆ -->
<button id="back_to_top"
  class="btn btn-accent fixed bottom-10 right-10 opacity-0 pointer-events-none transition-opacity duration-300 z-50 "
  title="Back to top" aria-label="Back to top" tabindex="0">
  <i class="fas fa-chevron-up"></i>
</button>

<script>
  (function(){
    const backToTopBtn = document.getElementById('back_to_top');

    // Êõ¥Êñ∞ÊåâÈíÆÈ¢úËâ≤Áî±daisyUIÁöÑbtnËá™Âä®ÁÆ°ÁêÜÔºåÊó†ÈúÄÈ¢ùÂ§ñ‰ª£Á†Å

    function toggleVisibility() {
      if(window.scrollY > 100) {
        backToTopBtn.classList.remove('opacity-0', 'pointer-events-none');
        backToTopBtn.classList.add('opacity-100', 'pointer-events-auto');
      } else {
        backToTopBtn.classList.remove('opacity-100', 'pointer-events-auto');
        backToTopBtn.classList.add('opacity-0', 'pointer-events-none');
      }
    }

    backToTopBtn.addEventListener('click', () => {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });

    backToTopBtn.addEventListener('keypress', (e) => {
      if(e.key === 'Enter' || e.key === ' ') {
        e.preventDefault();
        backToTopBtn.click();
      }
    });

    window.addEventListener('scroll', toggleVisibility);

    // È°µÈù¢Âä†ËΩΩÊó∂Âà§Êñ≠‰∏Ä‰∏ã
    toggleVisibility();
  })();
</script>


  <!-- MathJax for mathematical formulas -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        ignoreHtmlClass: 'tex2jax_ignore',
        processHtmlClass: 'tex2jax_process'
      },
      startup: {
        ready: () => {
          MathJax.startup.defaultReady();
        },
        pageReady: () => {
          MathJax.startup.defaultPageReady().then(() => {
            // Clean up $$ delimiters after rendering
            setTimeout(() => {
              const walker = document.createTreeWalker(
                document.querySelector('.post-content') || document.body,
                NodeFilter.SHOW_TEXT,
                null,
                false
              );
              
              let textNode;
              const nodesToUpdate = [];
              
              while (textNode = walker.nextNode()) {
                if (textNode.nodeValue.includes('$$') && 
                    textNode.parentNode.querySelector('mjx-container[display="true"]')) {
                  nodesToUpdate.push(textNode);
                }
              }
              
              nodesToUpdate.forEach(node => {
                node.nodeValue = node.nodeValue.replace(/\$\$/g, '');
              });
            }, 100);
          });
        }
      }
    };
  </script>


</body>
</html>
